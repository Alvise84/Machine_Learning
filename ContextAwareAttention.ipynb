{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPYDex9jZ0AEcdv+6hOrht9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alvise84/machine_learning/blob/main/ContextAwareAttention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ContextAware\n",
        "\n",
        "Проект, который объединяет различные виды механизма Attention для улучшения качества предсказаний и обучения."
      ],
      "metadata": {
        "id": "SZNpGZ0pZcQO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Шаг 1: Введение и импорт библиотек"
      ],
      "metadata": {
        "id": "O_OYuT9tZ23q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Введение\n",
        "\n",
        "В этом ноутбуке мы реализуем два вида механизма Attention: Multiplicative Attention и Additive Attention. Мы будем использовать библиотеку NumPy для вычислений и библиотеку Matplotlib для визуализации результатов.\n",
        "\n",
        "## Что такое Attention?\n",
        "\n",
        "Attention — это механизм, который позволяет модели фокусироваться на определенных частях входных данных при выполнении задачи. Это особенно полезно в задачах, где входные данные имеют различную важность для конечного результата.\n",
        "\n",
        "## Зачем нужен Attention?\n",
        "\n",
        "Attention помогает модели лучше понимать контекст и важность различных частей входных данных, что улучшает качество предсказаний и обучения.\n",
        "\n",
        "## Почему именно так?\n",
        "\n",
        "Мы используем NumPy для вычислений, так как это мощная библиотека для работы с массивами и матрицами. Matplotlib используется для визуализации, чтобы мы могли наглядно увидеть результаты.\n",
        "\n",
        "## Где это может использоваться на практике?\n",
        "\n",
        "Механизм Attention широко используется в различных областях, таких как:\n",
        "\n",
        "1. **Машинный перевод**: Attention позволяет модели фокусироваться на определенных частях предложения при переводе, что улучшает качество перевода.\n",
        "2. **Распознавание речи**: Attention помогает модели фокусироваться на определенных частях аудиосигнала, что улучшает точность распознавания речи.\n",
        "3. **Генерация текста**: Attention используется для генерации более качественного и контекстуально релевантного текста.\n",
        "4. **Анализ изображений**: Attention помогает модели фокусироваться на определенных частях изображения, что улучшает задачи классификации и сегментации изображений.\n",
        "5. **Рекомендательные системы**: Attention используется для улучшения качества рекомендаций, фокусируясь на наиболее релевантных элементах данных пользователя."
      ],
      "metadata": {
        "id": "zN1tu6mFaBDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "RWTu3ykIaC7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Шаг 2: Реализация функции Softmax"
      ],
      "metadata": {
        "id": "uL_f3blVadG9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Функция Softmax\n",
        "\n",
        "Функция Softmax используется для преобразования вектора значений в вектор вероятностей. Это важно для вычисления attention weights.\n",
        "\n",
        "Функция Softmax преобразует вектор значений в вектор вероятностей. Это необходимо для вычисления attention weights, которые используются для взвешивания входных данных. Softmax позволяет преобразовать любые значения в вероятности, что делает их удобными для использования в моделях машинного обучения."
      ],
      "metadata": {
        "id": "0BCl9p_YajXD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(vector):\n",
        "    '''\n",
        "    vector: np.array of shape (n, m)\n",
        "\n",
        "    return: np.array of shape (n, m)\n",
        "        Matrix where softmax is computed for every row independently\n",
        "    '''\n",
        "    nice_vector = vector - vector.max(axis=1, keepdims=True)\n",
        "    exp_vector = np.exp(nice_vector)\n",
        "    exp_denominator = np.sum(exp_vector, axis=1, keepdims=True)\n",
        "    softmax_ = exp_vector / exp_denominator\n",
        "    return softmax_"
      ],
      "metadata": {
        "id": "WyrqlmAkafrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Шаг 3: Реализация Multiplicative Attention"
      ],
      "metadata": {
        "id": "Q30ojzpGnnvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multiplicative Attention\n",
        "\n",
        "Multiplicative Attention использует матричное умножение для вычисления attention scores. Это позволяет работать с состояниями энкодера и декодера различных размерностей.\n",
        "\n",
        "Multiplicative Attention вычисляет attention scores с помощью матричного умножения. Это позволяет модели фокусироваться на определенных частях входных данных, что улучшает качество предсказаний. Матричное умножение позволяет эффективно вычислять attention scores для состояний энкодера и декодера различных размерностей."
      ],
      "metadata": {
        "id": "ZAy9BLQRnu_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def multiplicative_attention(decoder_hidden_state, encoder_hidden_states, W_mult):\n",
        "    '''\n",
        "    decoder_hidden_state: np.array of shape (n_features_dec, 1)\n",
        "    encoder_hidden_states: np.array of shape (n_features_enc, n_states)\n",
        "    W_mult: np.array of shape (n_features_dec, n_features_enc)\n",
        "\n",
        "    return: np.array of shape (n_features_enc, 1)\n",
        "        Final attention vector\n",
        "    '''\n",
        "    # Вычисление attention scores\n",
        "    attention_scores = np.dot(decoder_hidden_state.T, np.dot(W_mult, encoder_hidden_states))\n",
        "\n",
        "    # Вычисление attention weights\n",
        "    attention_weights = softmax(attention_scores)\n",
        "\n",
        "    # Вычисление итогового вектора\n",
        "    attention_vector = attention_weights.dot(encoder_hidden_states.T).T\n",
        "    return attention_vector\n"
      ],
      "metadata": {
        "id": "CQqpnXvOnrf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Шаг 4: Реализация Additive Attention"
      ],
      "metadata": {
        "id": "ojeF-VJRoEs1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Additive Attention\n",
        "\n",
        "Additive Attention использует нейронную сеть для вычисления attention scores. Это позволяет более гибко моделировать взаимодействие между состояниями энкодера и декодера.\n",
        "\n",
        "Additive Attention вычисляет attention scores с помощью нейронной сети. Это позволяет модели фокусироваться на определенных частях входных данных, что улучшает качество предсказаний. Нейронная сеть позволяет более гибко моделировать взаимодействие между состояниями энкодера и декодера."
      ],
      "metadata": {
        "id": "mttIawH_oJer"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def additive_attention(decoder_hidden_state, encoder_hidden_states, v_add, W_add_enc, W_add_dec):\n",
        "    '''\n",
        "    decoder_hidden_state: np.array of shape (n_features_dec, 1)\n",
        "    encoder_hidden_states: np.array of shape (n_features_enc, n_states)\n",
        "    v_add: np.array of shape (n_features_int, 1)\n",
        "    W_add_enc: np.array of shape (n_features_int, n_features_enc)\n",
        "    W_add_dec: np.array of shape (n_features_int, n_features_dec)\n",
        "\n",
        "    return: np.array of shape (n_features_enc, 1)\n",
        "        Final attention vector\n",
        "    '''\n",
        "    # Вычисление скрытых состояний\n",
        "    hidden_states = np.tanh(np.dot(W_add_enc, encoder_hidden_states) + np.dot(W_add_dec, decoder_hidden_state))\n",
        "\n",
        "    # Вычисление attention scores\n",
        "    attention_scores = np.dot(v_add.T, hidden_states)\n",
        "\n",
        "    # Вычисление attention weights\n",
        "    attention_weights = softmax(attention_scores)\n",
        "\n",
        "    # Вычисление итогового вектора\n",
        "    attention_vector = attention_weights.dot(encoder_hidden_states.T).T\n",
        "    return attention_vector"
      ],
      "metadata": {
        "id": "h3mioH4doGe_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Шаг 5: Пример использования функций"
      ],
      "metadata": {
        "id": "Q_SJkJaCoZig"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Пример использования функций\n",
        "\n",
        "Теперь мы продемонстрируем, как использовать реализованные функции для вычисления итоговых векторов с использованием Multiplicative и Additive Attention.\n",
        "\n",
        "Мы используем реализованные функции для вычисления итоговых векторов. Это позволяет нам увидеть, как работают механизмы Attention на практике. Примеры помогают лучше понять, как работают механизмы Attention и как они могут быть применены на практике."
      ],
      "metadata": {
        "id": "eAjqa9s1ofHL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Пример состояния декодера\n",
        "decoder_hidden_state = np.array([7, 11, 4]).astype(float)[:, None]\n",
        "\n",
        "# Пример состояний энкодера\n",
        "encoder_hidden_states_complex = np.array([[1, 5, 11, 4, -4], [7, 4, 1, 2, 2], [8, 12, 2, 11, 5], [-9, 0, 1, 8, 12]]).astype(float).T\n",
        "\n",
        "# Вектор весов v_add\n",
        "v_add = np.array([[-0.35, -0.58, 0.07, 1.39, -0.79, -1.78, -0.35]]).T\n",
        "\n",
        "# Матрица весов W_add_enc\n",
        "W_add_enc = np.array([\n",
        "    [-1.34, -0.1, -0.38, 0.12, -0.34],\n",
        "    [-1.0, 1.28, 0.49, -0.41, -0.32],\n",
        "    [-0.39, -1.38, 1.26, 1.21, 0.15],\n",
        "    [-0.18, 0.04, 1.36, -1.18, -0.53],\n",
        "    [-0.23, 0.96, 1.02, 0.39, -1.26],\n",
        "    [-1.27, 0.89, -0.85, -0.01, -1.19],\n",
        "    [0.46, -0.12, -0.86, -0.93, -0.4],\n",
        "])\n",
        "\n",
        "# Матрица весов W_add_dec\n",
        "W_add_dec = np.array([\n",
        "    [-1.62, -0.02, -0.39],\n",
        "    [0.43, 0.61, -0.23],\n",
        "    [-1.5, -0.43, -0.91],\n",
        "    [-0.14, 0.03, 0.05],\n",
        "    [0.85, 0.51, 0.63],\n",
        "    [0.39, -0.42, 1.34],\n",
        "    [-0.47, -0.31, -1.34],\n",
        "])\n",
        "\n",
        "# Матрица весов W_mult\n",
        "W_mult = np.array([\n",
        "    [-0.78, -0.97, -1.09, -1.79, 0.24],\n",
        "    [0.04, -0.27, -0.98, -0.49, 0.52],\n",
        "    [1.08, 0.91, -0.99, 2.04, -0.15],\n",
        "])\n",
        "\n",
        "# Вычисление итогового вектора с использованием Multiplicative Attention\n",
        "multiplicative_attention_vector = multiplicative_attention(decoder_hidden_state, encoder_hidden_states_complex, W_mult)\n",
        "print(\"Multiplicative Attention Vector:\", multiplicative_attention_vector)\n",
        "\n",
        "# Визуализация итогового вектора для Multiplicative Attention\n",
        "plt.figure(figsize=(2, 5))\n",
        "plt.pcolormesh(multiplicative_attention_vector, cmap=\"spring\")\n",
        "plt.colorbar()\n",
        "plt.title(\"Multiplicative Attention Vector\")\n",
        "plt.show()\n",
        "\n",
        "# Вычисление итогового вектора с использованием Additive Attention\n",
        "additive_attention_vector = additive_attention(decoder_hidden_state, encoder_hidden_states_complex, v_add, W_add_enc, W_add_dec)\n",
        "print(\"Additive Attention Vector:\", additive_attention_vector)\n",
        "\n",
        "# Визуализация итогового вектора для Additive Attention\n",
        "plt.figure(figsize=(2, 5))\n",
        "plt.pcolormesh(additive_attention_vector, cmap=\"spring\")\n",
        "plt.colorbar()\n",
        "plt.title(\"Additive Attention Vector\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QM1-Yi4Vobpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Шаг 6: Заключение"
      ],
      "metadata": {
        "id": "OqmDWsiHorYz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Заключение\n",
        "\n",
        "В этом ноутбуке мы реализовали и продемонстрировали работу двух видов механизма Attention: Multiplicative Attention и Additive Attention. Мы использовали библиотеку NumPy для вычислений и библиотеку Matplotlib для визуализации результатов. Эти механизмы позволяют модели фокусироваться на определенных частях входных данных, что улучшает качество предсказаний и обучения.\n",
        "\n",
        "### Где это может использоваться на практике?\n",
        "\n",
        "Механизм Attention широко используется в различных областях, таких как:\n",
        "\n",
        "1. **Машинный перевод**: Attention позволяет модели фокусироваться на определенных частях предложения при переводе, что улучшает качество перевода.\n",
        "2. **Распознавание речи**: Attention помогает модели фокусироваться на определенных частях аудиосигнала, что улучшает точность распознавания речи.\n",
        "3. **Генерация текста**: Attention используется для генерации более качественного и контекстуально релевантного текста.\n",
        "4. **Анализ изображений**: Attention помогает модели фокусироваться на определенных частях изображения, что улучшает задачи классификации и сегментации изображений.\n",
        "5. **Рекомендательные системы**: Attention используется для улучшения качества рекомендаций, фокусируясь на наиболее релевантных элементах данных пользователя.\n"
      ],
      "metadata": {
        "id": "HlYQTaXBoybX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Шаг 7: Сохранение функций в файл"
      ],
      "metadata": {
        "id": "-CI1hqnIo77M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Сохранение функций в файл\n",
        "\n",
        "Теперь мы сохраним реализованные функции в файл `Attention_ML.py`."
      ],
      "metadata": {
        "id": "7i1oYlHSpAPv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Создание словаря с функциями\n",
        "out_dict = {\n",
        "    'multiplicative_attention': multiplicative_attention,\n",
        "    'additive_attention': additive_attention\n",
        "}\n",
        "\n",
        "# Сохранение словаря в файл\n",
        "np.save(\"submission_dict_hw08.npy\", out_dict, allow_pickle=True)\n",
        "print(\"File saved to `submission_dict_hw08.npy`\")\n",
        "\n",
        "# Сохранение функций в файл Attention_ML.py\n",
        "with open(\"Attention_ML.py\", \"w\") as f:\n",
        "    f.write(\"import numpy as np\\n\\n\")\n",
        "    f.write(\"def softmax(vector):\\n\")\n",
        "    f.write(\"    '''\\n\")\n",
        "    f.write(\"    vector: np.array of shape (n, m)\\n\\n\")\n",
        "    f.write(\"    return: np.array of shape (n, m)\\n\")\n",
        "    f.write(\"        Matrix where softmax is computed for every row independently\\n\")\n",
        "    f.write(\"    '''\\n\")\n",
        "    f.write(\"    nice_vector = vector - vector.max(axis=1, keepdims=True)\\n\")\n",
        "    f.write(\"    exp_vector = np.exp(nice_vector)\\n\")\n",
        "    f.write(\"    exp_denominator = np.sum(exp_vector, axis=1, keepdims=True)\\n\")\n",
        "    f.write(\"    softmax_ = exp_vector / exp_denominator\\n\")\n",
        "    f.write(\"    return softmax_\\n\\n\")\n",
        "\n",
        "    f.write(\"def multiplicative_attention(decoder_hidden_state, encoder_hidden_states, W_mult):\\n\")\n",
        "    f.write(\"    '''\\n\")\n",
        "    f.write(\"    decoder_hidden_state: np.array of shape (n_features_dec, 1)\\n\")\n",
        "    f.write(\"    encoder_hidden_states: np.array of shape (n_features_enc, n_states)\\n\")\n",
        "    f.write(\"    W_mult: np.array of shape (n_features_dec, n_features_enc)\\n\\n\")\n",
        "    f.write(\"    return: np.array of shape (n_features_enc, 1)\\n\")\n",
        "    f.write(\"        Final attention vector\\n\")\n",
        "    f.write(\"    '''\\n\")\n",
        "    f.write(\"    # Вычисление attention scores\\n\")\n",
        "    f.write(\"    attention_scores = np.dot(decoder_hidden_state.T, np.dot(W_mult, encoder_hidden_states))\\n\\n\")\n",
        "    f.write(\"    # Вычисление attention weights\\n\")\n",
        "    f.write(\"    attention_weights = softmax(attention_scores)\\n\\n\")\n",
        "    f.write(\"    # Вычисление итогового вектора\\n\")\n",
        "    f.write(\"    attention_vector = attention_weights.dot(encoder_hidden_states.T).T\\n\")\n",
        "    f.write(\"    return attention_vector\\n\\n\")\n",
        "\n",
        "    f.write(\"def additive_attention(decoder_hidden_state, encoder_hidden_states, v_add, W_add_enc, W_add_dec):\\n\")\n",
        "    f.write(\"    '''\\n\")\n",
        "    f.write(\"    decoder_hidden_state: np.array of shape (n_features_dec, 1)\\n\")\n",
        "    f.write(\"    encoder_hidden_states: np.array of shape (n_features_enc, n_states)\\n\")\n",
        "    f.write(\"    v_add: np.array of shape (n_features_int, 1)\\n\")\n",
        "    f.write(\"    W_add_enc: np.array of shape (n_features_int, n_features_enc)\\n\")\n",
        "    f.write(\"    W_add_dec: np.array of shape (n_features_int, n_features_dec)\\n\\n\")\n",
        "    f.write(\"    return: np.array of shape (n_features_enc, 1)\\n\")\n",
        "    f.write(\"        Final attention vector\\n\")\n",
        "    f.write(\"    '''\\n\")\n",
        "    f.write(\"    # Вычисление скрытых состояний\\n\")\n",
        "    f.write(\"    hidden_states = np.tanh(np.dot(W_add_enc, encoder_hidden_states) + np.dot(W_add_dec, decoder_hidden_state))\\n\\n\")\n",
        "    f.write(\"    # Вычисление attention scores\\n\")\n",
        "    f.write(\"    attention_scores = np.dot(v_add.T, hidden_states)\\n\\n\")\n",
        "    f.write(\"    # Вычисление attention weights\\n\")\n",
        "    f.write(\"    attention_weights = softmax(attention_scores)\\n\\n\")\n",
        "    f.write(\"    # Вычисление итогового вектора\\n\")\n",
        "    f.write(\"    attention_vector = attention_weights.dot(encoder_hidden_states.T).T\\n\")\n",
        "    f.write(\"    return attention_vector\\n\")\n"
      ],
      "metadata": {
        "id": "jFvhoWdoovjP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}